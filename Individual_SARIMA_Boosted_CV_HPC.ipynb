{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no = 0, no_preds=30): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = sys.argv[1], station = sys.argv[2], model_no = sys.argv[3], no_preds=sys.argv[4]): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n"
     ]
    }
   ],
   "source": [
    "import glob,subprocess\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "combinations = [[1,0], [1,1], [2,0]]\n",
    "cmd = '''hi\n",
    "safa'''\n",
    "for straekning, station in combinations:\n",
    "    cmdi = cmd + f' {straekning} {station} 0 30'\n",
    "    proc = subprocess.Popen('bsub ', stdin=subprocess.PIPE, shell=True)\n",
    "    proc.communicate(bytes(cmdi, encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval('[(3, 1, 5), (0, 0, 0, 0)]')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Final (ARIMA + Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(straekning = sys.argv[1], station = sys.argv[2], model_no = sys.argv[3], no_preds=sys.argv[4]): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tarima_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\tarima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with ARIMA model\n",
    "\t\t\tarima_pred = arima_model.predict(n_periods=len(test))\n",
    "\t\t\tarima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\tarima_residuals_train = arima_pred - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\tarima_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tarima_pred.append(arima_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_arima = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\tarima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Best model parameters for ARIMA\n",
    "\tmodel_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "\tbest_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(straekning), int(station)))]['Values'].values[0])\n",
    "\n",
    "\t# Define models\n",
    "\tarima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "\t\t\t\t  'CV_score_arima': cv_score_arima}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/({}, {})_SARIMA_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/({}, {})_SARIMA_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caller\n",
    "cmd = '''\n",
    "#!/bin/sh\n",
    "#BSUB -J DSB_SARIMA_submit\n",
    "#BSUB -o DSB_SARIMA_submit%J.out\n",
    "#BSUB -e DSB_SARIMA_submit%J.err\n",
    "#BSUB -n 4\n",
    "#BSUB -R \"rusage[mem=10G]\"\n",
    "#BSUB -R \"span[hosts=1]\"\n",
    "#BSUB -W 2:00 \n",
    "##BSUB -u s214659@dtu.dk\n",
    "### -- send notification at start --\n",
    "#BSUB -B\n",
    "### -- send notification at completion--\n",
    "#BSUB -N\n",
    "#BSUB -o Output_%J.out \n",
    "#BSUB -e Error_%J.err \n",
    "# end of BSUB options\n",
    "\n",
    "# load a scipy module\n",
    "# replace VERSION and uncomment\n",
    "# module load scipy\n",
    "module load python3/3.10.13\n",
    "# activate the virtual environment \n",
    "# NOTE: needs to have been built with the same SciPy version above!\n",
    "source DSB_env/bin/activate\n",
    "### python tester.py\n",
    "python Individual_SARIMA_Boosted_CV1.py'''\n",
    "import glob,subprocess\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "combinations = [[1,0], [1,1], [2,0]]\n",
    "\n",
    "for straekning, station in combinations:\n",
    "    cmdi = cmd + ' {} {} {} {}'.format(straekning,station,0,30)\n",
    "    #print(cmdi)\n",
    "    proc = subprocess.Popen('bsub ', stdin=subprocess.PIPE, shell=True)\n",
    "    #proc.communicate(bytes(cmdi, encoding='UTF-8'))\n",
    "    proc.communicate(cmdi.encode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True final (Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Expanding Window CV\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(straekning, station, model_no, no_preds): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tboosted_model = estimator_tuple\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], y[train])\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\t\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), full_score,  cv_scores, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, error_full, cv_score_full, feature_importances = custom_cross_val_predict((boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Error_full': error_full, 'CV_score_full': cv_score_full}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/({}, {})_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/({}, {})_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))\n",
    "HPC_SARIMA_Boosted(straekning = 2, station = 0, model_no = 0, no_preds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Final (HMM + Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Expanding Window CV\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(straekning, station, model_no, no_preds): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\thmm_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\thmm_model.fit(y[train].reshape(-1,1)) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with HMM model\n",
    "\t\t\t### NOTE: ONLY ONE STEP - NEEDS LOOP If more\n",
    "\t\t\tlast_state = hmm_model.predict(y[train].reshape(-1,1))[-1]\n",
    "\t\t\t# State transition matrix\n",
    "\t\t\tnext_state_probs = hmm_model.transmat_[last_state]\n",
    "\t\t\tnext_state = np.argmax(next_state_probs)\n",
    "            # Mean and variance of next state\n",
    "\t\t\tmean_next_state = hmm_model.means_[next_state]\n",
    "\t\t\tcovar_next_state = hmm_model.covars_[next_state]\n",
    "\t\t\t#hmm_pred = np.random.multivariate_normal(mean_next_state.ravel(), covar_next_state)\n",
    "\t\t\thmm_pred = [np.mean([mean_next_state, y[train].reshape(-1,1)[-1]])] # The mean and the latest value -> smaller error.\n",
    "\t\t\t\n",
    "\t\t\thmm_pred_cap = max(min(1, hmm_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\thmm_residuals_train = hmm_pred - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], hmm_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, hmm_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, hmm_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\thmm_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, hmm_block, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\thmm_pred.append(hmm_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_hmm = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_hmm_score = float(abs(y[test] - hmm_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_hmm.append(fold_hmm_score)\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\thmm_score = np.mean(cv_scores_hmm)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), np.array(hmm_pred), full_score,  hmm_score, cv_scores, cv_scores_hmm, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tnp.random.seed(22)\n",
    "\thmm_model = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", algorithm='viterbi')\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, pred_hmm, error_full, error_hmm, cv_score_full, cv_score_hmm, feature_importances = custom_cross_val_predict((hmm_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Predictions_hmm': pred_hmm, 'Error_full': error_full, 'Error_hmm': error_hmm, 'CV_score_full': cv_score_full, \n",
    "\t\t\t\t  'CV_score_hmm': cv_score_hmm}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/({}, {})_HMM_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/({}, {})_HMM_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))\n",
    "HPC_SARIMA_Boosted(straekning = 2, station = 0, model_no = 0, no_preds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSB_Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
