{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no = 0, no_preds=30): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = sys.argv[1], station = sys.argv[2], model_no = sys.argv[3], no_preds=sys.argv[4]): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n"
     ]
    }
   ],
   "source": [
    "import glob,subprocess\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "combinations = [[1,0], [1,1], [2,0]]\n",
    "cmd = '''hi\n",
    "safa'''\n",
    "for straekning, station in combinations:\n",
    "    cmdi = cmd + f' {straekning} {station} 0 30'\n",
    "    proc = subprocess.Popen('bsub ', stdin=subprocess.PIPE, shell=True)\n",
    "    proc.communicate(bytes(cmdi, encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval('[(3, 1, 5), (0, 0, 0, 0)]')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Final (ARIMA + Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(straekning = sys.argv[1], station = sys.argv[2], model_no = sys.argv[3], no_preds=sys.argv[4]): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tarima_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\tarima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with ARIMA model\n",
    "\t\t\tarima_pred = arima_model.predict(n_periods=len(test))\n",
    "\t\t\tarima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\tarima_residuals_train = arima_pred - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\tarima_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tarima_pred.append(arima_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_arima = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\tarima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Best model parameters for ARIMA\n",
    "\tmodel_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "\tbest_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(straekning), int(station)))]['Values'].values[0])\n",
    "\n",
    "\t# Define models\n",
    "\tarima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "\t\t\t\t  'CV_score_arima': cv_score_arima}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/({}, {})_SARIMA_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/({}, {})_SARIMA_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caller\n",
    "cmd = '''\n",
    "#!/bin/sh\n",
    "#BSUB -J DSB_SARIMA_submit\n",
    "#BSUB -o DSB_SARIMA_submit%J.out\n",
    "#BSUB -e DSB_SARIMA_submit%J.err\n",
    "#BSUB -n 4\n",
    "#BSUB -R \"rusage[mem=10G]\"\n",
    "#BSUB -R \"span[hosts=1]\"\n",
    "#BSUB -W 2:00 \n",
    "##BSUB -u s214659@dtu.dk\n",
    "### -- send notification at start --\n",
    "#BSUB -B\n",
    "### -- send notification at completion--\n",
    "#BSUB -N\n",
    "#BSUB -o Output_%J.out \n",
    "#BSUB -e Error_%J.err \n",
    "# end of BSUB options\n",
    "\n",
    "# load a scipy module\n",
    "# replace VERSION and uncomment\n",
    "# module load scipy\n",
    "module load python3/3.10.13\n",
    "# activate the virtual environment \n",
    "# NOTE: needs to have been built with the same SciPy version above!\n",
    "source DSB_env/bin/activate\n",
    "### python tester.py\n",
    "python Individual_SARIMA_Boosted_CV1.py'''\n",
    "import glob,subprocess\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "combinations = [[1,0], [1,1], [2,0]]\n",
    "\n",
    "for straekning, station in combinations:\n",
    "    cmdi = cmd + ' {} {} {} {}'.format(straekning,station,0,30)\n",
    "    #print(cmdi)\n",
    "    proc = subprocess.Popen('bsub ', stdin=subprocess.PIPE, shell=True)\n",
    "    #proc.communicate(bytes(cmdi, encoding='UTF-8'))\n",
    "    proc.communicate(cmdi.encode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True final (Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Expanding Window CV\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_Boosted(straekning, station, model_no, no_preds): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tboosted_model = estimator_tuple\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], y[train])\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\t\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), full_score,  cv_scores, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, error_full, cv_score_full, feature_importances = custom_cross_val_predict((boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Error_full': error_full, 'CV_score_full': cv_score_full}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/Xgboost/({}, {})_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/Xgboost/({}, {})_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))\n",
    "HPC_Boosted(straekning = 2, station = 0, model_no = 0, no_preds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Final (HMM + Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Expanding Window CV\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(straekning, station, model_no, no_preds): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\thmm_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\thmm_model.fit(y[train].reshape(-1,1)) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with HMM model\n",
    "\t\t\t### NOTE: ONLY ONE STEP - NEEDS LOOP If more\n",
    "\t\t\tlast_state = hmm_model.predict(y[train].reshape(-1,1))[-1]\n",
    "\t\t\t# State transition matrix\n",
    "\t\t\tnext_state_probs = hmm_model.transmat_[last_state]\n",
    "\t\t\tnext_state = np.argmax(next_state_probs)\n",
    "            # Mean and variance of next state\n",
    "\t\t\tmean_next_state = hmm_model.means_[next_state]\n",
    "\t\t\tcovar_next_state = hmm_model.covars_[next_state]\n",
    "\t\t\t#hmm_pred = np.random.multivariate_normal(mean_next_state.ravel(), covar_next_state)\n",
    "\t\t\thmm_pred = [np.mean([mean_next_state, y[train].reshape(-1,1)[-1]])] # The mean and the latest value -> smaller error.\n",
    "\t\t\t\n",
    "\t\t\thmm_pred_cap = max(min(1, hmm_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\thmm_residuals_train = hmm_pred - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], hmm_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, hmm_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, hmm_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\thmm_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, hmm_block, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\thmm_pred.append(hmm_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_hmm = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_hmm_score = float(abs(y[test] - hmm_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_hmm.append(fold_hmm_score)\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\thmm_score = np.mean(cv_scores_hmm)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), np.array(hmm_pred), full_score,  hmm_score, cv_scores, cv_scores_hmm, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tnp.random.seed(22)\n",
    "\thmm_model = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", algorithm='viterbi')\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, pred_hmm, error_full, error_hmm, cv_score_full, cv_score_hmm, feature_importances = custom_cross_val_predict((hmm_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Predictions_hmm': pred_hmm, 'Error_full': error_full, 'Error_hmm': error_hmm, 'CV_score_full': cv_score_full, \n",
    "\t\t\t\t  'CV_score_hmm': cv_score_hmm}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/({}, {})_HMM_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/({}, {})_HMM_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))\n",
    "HPC_SARIMA_Boosted(straekning = 2, station = 0, model_no = 0, no_preds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All options in one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running... \n",
      "Both ARIMA and Boosted Random Forest: True \n",
      "Boosted Random Forest Model: Catboost \n",
      "Dataset: Cleaned_simple_lagged\n",
      "No. predictions: 10\n",
      "----------------\n",
      "Initialize Expanding Window CV\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'Results/Separate_Runs/Cleaned_simple_lagged/SARIMA_Catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 274\u001b[0m\n\u001b[1;32m    271\u001b[0m \tfi_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults/Separate_Runs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)_Boosted\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_feature_importances\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(data_name, model_name, straekning, station, model_no, no_preds), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m#HPC_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]), data_name=sys.argv[5], both=sys.argv[6])\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[43mHPC_Boosted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstraekning\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_no\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCleaned_simple_lagged\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 270\u001b[0m, in \u001b[0;36mHPC_Boosted\u001b[0;34m(straekning, station, model_no, no_preds, data_name, both)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m \tresult_df, fi_df \u001b[38;5;241m=\u001b[39m custom_cross_val_predict_Boosted((boosted_model), y, X, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, averaging\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_raw_predictions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, initial\u001b[38;5;241m=\u001b[39minitial_start)\n\u001b[0;32m--> 270\u001b[0m \u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mResults/Separate_Runs/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m/(\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m)_Boosted\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m_results\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstraekning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_no\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m fi_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults/Separate_Runs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/(\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)_Boosted\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_feature_importances\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(data_name, model_name, straekning, station, model_no, no_preds), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DSB_Bachelorprojekt/lib/python3.11/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DSB_Bachelorprojekt/lib/python3.11/site-packages/pandas/core/generic.py:3961\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3950\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3952\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3953\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3954\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3958\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3959\u001b[0m )\n\u001b[0;32m-> 3961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3964\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DSB_Bachelorprojekt/lib/python3.11/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/anaconda3/envs/DSB_Bachelorprojekt/lib/python3.11/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/anaconda3/envs/DSB_Bachelorprojekt/lib/python3.11/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DSB_Bachelorprojekt/lib/python3.11/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'Results/Separate_Runs/Cleaned_simple_lagged/SARIMA_Catboost'"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_Boosted(straekning, station, model_no, no_preds, data_name, both): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict_Boosted(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tbaseline = np.mean(y[train])\n",
    "\t\t\t\n",
    "\t\t\tboosted_model = estimator_tuple\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], y[train])\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, baseline, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\ty_true = []\n",
    "\t\tbaseline_pred = []\n",
    "\t\tfeature_importances = np.zeros(prediction_blocks[0][3].shape)\n",
    "\t\tfor i, (pred_block, test_indices, baseline, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\t\t\tbaseline_pred += [baseline]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_baseline = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_baseline.append(float(abs(y[test] - baseline_pred[fold])))\n",
    "\t\t\t\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\tbaseline_score = np.mean(cv_scores_baseline)\n",
    "\n",
    "\t\tresult_dictionary = {'y_true': y_true,'Predictions_full': avgfunc(predictions, axis=1), 'Predictions_baseline': baseline_pred, 'Error_full': full_score, 'Error_baseline': baseline_score, 'CV_score_full': cv_scores, 'CV_score_baseline': cv_scores_baseline}\n",
    "\n",
    "\t\t# Save results in .csv's\n",
    "\t\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\t\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\n",
    "\t\treturn result_df, fi_df\n",
    "\t\n",
    "\tdef custom_cross_val_predict_SARIMA_Boosted(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tbaseline = np.mean(y[train])\n",
    "\t\t\t\n",
    "\t\t\tarima_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\tarima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with ARIMA model\n",
    "\t\t\tarima_pred = arima_model.predict(n_periods=len(test))\n",
    "\t\t\tarima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\tarima_residuals_train = arima_pred - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, arima_pred_cap, baseline, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\tarima_pred = []\n",
    "\t\tbaseline_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros(prediction_blocks[0][4].shape)\n",
    "\t\tfor i, (pred_block, test_indices, arima_block, baseline, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tarima_pred.append(arima_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\tbaseline_pred += [baseline]\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_arima = []\n",
    "\t\tcv_scores_baseline =[]\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_arima.append(fold_arima_score)\n",
    "\t\t\tcv_scores_baseline.append(float(abs(y[test] - baseline_pred[fold])))\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\tarima_score = np.mean(cv_scores_arima)\n",
    "\t\tbaseline_score = np.mean(cv_scores_baseline)\n",
    "\t\tresult_dictionary = {'y_true': y_true,'Predictions_full': avgfunc(predictions, axis=1), 'Predictions_arima': np.array(arima_pred), 'Predictions_baseline': baseline_pred, 'Error_full': full_score, 'Error_arima': arima_score, 'Error_baseline': baseline_score, 'CV_score_full': cv_scores, 'CV_score_arima': cv_scores_arima, 'CV_score_baseline':cv_scores_baseline}\n",
    "\t\t# Save results in .csv's\n",
    "\t\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\t\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\t\t\n",
    "\t\treturn result_df, fi_df\n",
    "\n",
    "\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/{}_data.csv\".format(data_name))\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tmodel_name = 'Xgboost'\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tmodel_name = 'Catboost'\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\tprint(f'Running... \\nBoth ARIMA and Boosted Random Forest: {both} \\nBoosted Random Forest Model: {model_name} \\nDataset: {data_name}\\nNo. predictions: {no_preds}')\n",
    "\tprint('----------------')\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\n",
    "\tif both:\n",
    "\t\tmodel_name = 'SARIMA_' + model_name\n",
    "\t\tmodel_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "\t\tbest_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(straekning), int(station)))]['Values'].values[0])\n",
    "\t\t# Define models\n",
    "\t\tarima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "\t\tresult_df, fi_df = custom_cross_val_predict_SARIMA_Boosted((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\t\t\n",
    "\telse:\n",
    "\t\tresult_df, fi_df = custom_cross_val_predict_Boosted((boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\t\n",
    "\tresult_df.to_csv('Results/Separate_Runs/{}/{}/({}, {})_Boosted{}_results{}.csv'.format(data_name, model_name, straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/{}/{}/({}, {})_Boosted{}_feature_importances{}.csv'.format(data_name, model_name, straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]), data_name=sys.argv[5], both=sys.argv[6])\n",
    "HPC_Boosted(straekning = 20, station = 19, model_no = 1, no_preds=10, data_name='Cleaned_simple_lagged', both=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [[2,0],\n",
    "[2,2],\n",
    "[9,2],\n",
    "[10,3],\n",
    "[24,3],\n",
    "[21,4],\n",
    "[10,5],\n",
    "[8,5],\n",
    "[11,6],\n",
    "[13,6],\n",
    "[9,6],\n",
    "[2,8],\n",
    "[20,9],\n",
    "[17,11],\n",
    "[5,12],\n",
    "[5,13],\n",
    "[6,13],\n",
    "[7,13],\n",
    "[19,14],\n",
    "[17,15],\n",
    "[11,16],\n",
    "[19,17],\n",
    "[20,17],\n",
    "[21,17],\n",
    "[27,17],\n",
    "[23,18],\n",
    "[20,19],\n",
    "[15,20],\n",
    "[16,20],\n",
    "[2,21],\n",
    "[3,21],\n",
    "[18,22],\n",
    "[23,22],\n",
    "[25,22],\n",
    "[25,23],\n",
    "[13,24],\n",
    "[15,24],\n",
    "[13,25],\n",
    "[14,25],\n",
    "[20,26],\n",
    "[11,27],\n",
    "[24,28],\n",
    "[14,29],\n",
    "[16,31],\n",
    "[18,31],\n",
    "[27,31],\n",
    "[16,33],\n",
    "[17,33],\n",
    "[19,33],\n",
    "[6,34],\n",
    "[9,34],\n",
    "[12,35],\n",
    "[16,36],\n",
    "[7,37],\n",
    "[8,37],\n",
    "[6,38],\n",
    "[3,39],\n",
    "[4,39],\n",
    "[5,39],\n",
    "[14,40],\n",
    "[24,41],\n",
    "[11,42],\n",
    "[12,42],\n",
    "[4,43],\n",
    "[8,44],\n",
    "[3,45],\n",
    "[5,46],\n",
    "[9,46],\n",
    "[25,47]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSB_Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
