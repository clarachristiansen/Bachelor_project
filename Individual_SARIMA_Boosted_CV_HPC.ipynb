{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no = 0, no_preds=30): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = sys.argv[1], station = sys.argv[2], model_no = sys.argv[3], no_preds=sys.argv[4]): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n"
     ]
    }
   ],
   "source": [
    "import glob,subprocess\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "combinations = [[1,0], [1,1], [2,0]]\n",
    "cmd = '''hi\n",
    "safa'''\n",
    "for straekning, station in combinations:\n",
    "    cmdi = cmd + f' {straekning} {station} 0 30'\n",
    "    proc = subprocess.Popen('bsub ', stdin=subprocess.PIPE, shell=True)\n",
    "    proc.communicate(bytes(cmdi, encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval('[(3, 1, 5), (0, 0, 0, 0)]')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Final (ARIMA + Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(straekning = sys.argv[1], station = sys.argv[2], model_no = sys.argv[3], no_preds=sys.argv[4]): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tarima_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\tarima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with ARIMA model\n",
    "\t\t\tarima_pred = arima_model.predict(n_periods=len(test))\n",
    "\t\t\tarima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\tarima_residuals_train = arima_pred - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\tarima_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tarima_pred.append(arima_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_arima = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\tarima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Best model parameters for ARIMA\n",
    "\tmodel_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "\tbest_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(straekning), int(station)))]['Values'].values[0])\n",
    "\n",
    "\t# Define models\n",
    "\tarima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "\t\t\t\t  'CV_score_arima': cv_score_arima}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/({}, {})_SARIMA_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/({}, {})_SARIMA_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caller\n",
    "cmd = '''\n",
    "#!/bin/sh\n",
    "#BSUB -J DSB_SARIMA_submit\n",
    "#BSUB -o DSB_SARIMA_submit%J.out\n",
    "#BSUB -e DSB_SARIMA_submit%J.err\n",
    "#BSUB -n 4\n",
    "#BSUB -R \"rusage[mem=10G]\"\n",
    "#BSUB -R \"span[hosts=1]\"\n",
    "#BSUB -W 2:00 \n",
    "##BSUB -u s214659@dtu.dk\n",
    "### -- send notification at start --\n",
    "#BSUB -B\n",
    "### -- send notification at completion--\n",
    "#BSUB -N\n",
    "#BSUB -o Output_%J.out \n",
    "#BSUB -e Error_%J.err \n",
    "# end of BSUB options\n",
    "\n",
    "# load a scipy module\n",
    "# replace VERSION and uncomment\n",
    "# module load scipy\n",
    "module load python3/3.10.13\n",
    "# activate the virtual environment \n",
    "# NOTE: needs to have been built with the same SciPy version above!\n",
    "source DSB_env/bin/activate\n",
    "### python tester.py\n",
    "python Individual_SARIMA_Boosted_CV1.py'''\n",
    "import glob,subprocess\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "combinations = [[1,0], [1,1], [2,0]]\n",
    "\n",
    "for straekning, station in combinations:\n",
    "    cmdi = cmd + ' {} {} {} {}'.format(straekning,station,0,30)\n",
    "    #print(cmdi)\n",
    "    proc = subprocess.Popen('bsub ', stdin=subprocess.PIPE, shell=True)\n",
    "    #proc.communicate(bytes(cmdi, encoding='UTF-8'))\n",
    "    proc.communicate(cmdi.encode('UTF-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True final (Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Expanding Window CV\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_Boosted(straekning, station, model_no, no_preds): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tboosted_model = estimator_tuple\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], y[train])\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\t\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), full_score,  cv_scores, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, error_full, cv_score_full, feature_importances = custom_cross_val_predict((boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Error_full': error_full, 'CV_score_full': cv_score_full}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/Xgboost/({}, {})_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/Xgboost/({}, {})_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))\n",
    "HPC_Boosted(straekning = 2, station = 0, model_no = 0, no_preds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Final (HMM + Boosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Expanding Window CV\n",
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Even though the 'means_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'm'\n",
      "Even though the 'covars_' attribute is set, it will be overwritten during initialization because 'init_params' contains 'c'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(straekning, station, model_no, no_preds): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\t\t\"\"\"Generate cross-validated estimates for each input data point\n",
    "\t\t\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\testimator : tuple\n",
    "\t\t\tA tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "\t\t\tand the second one should be the Random Forest model.\n",
    "\n",
    "\t\ty : array-like or iterable, shape=(n_samples,)\n",
    "\t\t\tThe time-series array.\n",
    "\n",
    "\t\tX : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "\t\t\tAn optional 2-d array of exogenous variables.\n",
    "\n",
    "\t\tcv : BaseTSCrossValidator or None, optional (default=None)\n",
    "\t\t\tAn instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "\t\t\tNote that for cross-validation predictions, the CV step cannot exceed\n",
    "\t\t\tthe CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "\t\tverbose : integer, optional\n",
    "\t\t\tThe verbosity level.\n",
    "\n",
    "\t\taveraging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "\t\t\tUnlike normal CV, time series CV might have different folds (windows)\n",
    "\t\t\tforecasting the same time step. After all forecast windows are made,\n",
    "\t\t\twe build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "\t\t\tso::\n",
    "\n",
    "\t\t\t\tnan nan nan  # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t  1 nan nan  # test samples\n",
    "\t\t\t\t  4   3 nan\n",
    "\t\t\t\t  3 2.5 3.5\n",
    "\t\t\t\tnan   6   5\n",
    "\t\t\t\tnan nan   4\n",
    "\n",
    "\t\t\tWe then average each time step's forecasts to end up with our final\n",
    "\t\t\tprediction results.\n",
    "\n",
    "\t\treturn_raw_predictions : bool (default=False)\n",
    "\t\t\tIf True, raw predictions are returned instead of averaged ones.\n",
    "\t\t\tThis results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "\t\t\t\tnan nan nan # training samples\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\t1   4   2   # test samples\n",
    "\t\t\t\t2   5   7\n",
    "\t\t\t\t8   9   1\n",
    "\t\t\t\tnan nan nan\n",
    "\t\t\t\tnan nan nan\n",
    "\n",
    "\t\t\tFirst column contains all one-step-ahead-predictions, second column all\n",
    "\t\t\ttwo-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "\t\t\tas desired.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tpredictions : array-like, shape=(n_samples,)\n",
    "\t\t\tThe predicted values.\n",
    "\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\thmm_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\thmm_model.fit(y[train].reshape(-1,1)) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with HMM model\n",
    "\t\t\t### NOTE: ONLY ONE STEP - NEEDS LOOP If more\n",
    "\t\t\tlast_state = hmm_model.predict(y[train].reshape(-1,1))[-1]\n",
    "\t\t\t# State transition matrix\n",
    "\t\t\tnext_state_probs = hmm_model.transmat_[last_state]\n",
    "\t\t\tnext_state = np.argmax(next_state_probs)\n",
    "            # Mean and variance of next state\n",
    "\t\t\tmean_next_state = hmm_model.means_[next_state]\n",
    "\t\t\tcovar_next_state = hmm_model.covars_[next_state]\n",
    "\t\t\t#hmm_pred = np.random.multivariate_normal(mean_next_state.ravel(), covar_next_state)\n",
    "\t\t\thmm_pred = [np.mean([mean_next_state, y[train].reshape(-1,1)[-1]])] # The mean and the latest value -> smaller error.\n",
    "\t\t\t\n",
    "\t\t\thmm_pred_cap = max(min(1, hmm_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\thmm_residuals_train = hmm_pred - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], hmm_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, hmm_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, hmm_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\thmm_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros((211,))\n",
    "\t\tfor i, (pred_block, test_indices, hmm_block, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\thmm_pred.append(hmm_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_hmm = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_hmm_score = float(abs(y[test] - hmm_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_hmm.append(fold_hmm_score)\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\thmm_score = np.mean(cv_scores_hmm)\n",
    "\n",
    "\t\treturn y_true, avgfunc(predictions, axis=1), np.array(hmm_pred), full_score,  hmm_score, cv_scores, cv_scores_hmm, feature_importances\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tnp.random.seed(22)\n",
    "\thmm_model = hmm.GaussianHMM(n_components=10, covariance_type=\"full\", algorithm='viterbi')\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\ty_true, pred_full, pred_hmm, error_full, error_hmm, cv_score_full, cv_score_hmm, feature_importances = custom_cross_val_predict((hmm_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tresult_dictionary = {'y_true': y_true,'Predictions_full': pred_full, 'Predictions_hmm': pred_hmm, 'Error_full': error_full, 'Error_hmm': error_hmm, 'CV_score_full': cv_score_full, \n",
    "\t\t\t\t  'CV_score_hmm': cv_score_hmm}\n",
    "\n",
    "\t# Save results in .csv's\n",
    "\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\tresult_df.to_csv('Results/Separate_Runs/({}, {})_HMM_Boosted{}_results{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/({}, {})_HMM_Boosted{}_feature_importances{}.csv'.format(straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_SARIMA_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]))\n",
    "HPC_SARIMA_Boosted(straekning = 2, station = 0, model_no = 0, no_preds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All options in one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running... \n",
      "Both ARIMA and Boosted Random Forest: False \n",
      "Boosted Random Forest Model: Catboost \n",
      "Dataset: Cleaned_simple_lagged\n",
      "No. predictions: 10\n",
      "----------------\n",
      "Initialize Expanding Window CV\n",
      "HO\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "   y_true  Predictions_full  Predictions_baseline  Error_full  Error_baseline  \\\n",
      "0   0.834          0.755976              0.822660    0.079303        0.078479   \n",
      "1   0.760          0.778434              0.822665    0.079303        0.078479   \n",
      "2   0.921          0.820290              0.822640    0.079303        0.078479   \n",
      "3   0.926          0.853453              0.822679    0.079303        0.078479   \n",
      "4   0.941          0.855831              0.822719    0.079303        0.078479   \n",
      "5   0.674          0.843515              0.822766    0.079303        0.078479   \n",
      "6   0.849          0.798759              0.822707    0.079303        0.078479   \n",
      "7   0.888          0.823909              0.822718    0.079303        0.078479   \n",
      "8   0.900          0.808453              0.822743    0.079303        0.078479   \n",
      "9   0.896          0.833245              0.822773    0.079303        0.078479   \n",
      "\n",
      "   CV_score_full  CV_score_baseline  \n",
      "0       0.078024           0.011340  \n",
      "1       0.018434           0.062665  \n",
      "2       0.100710           0.098360  \n",
      "3       0.072547           0.103321  \n",
      "4       0.085169           0.118281  \n",
      "5       0.169515           0.148766  \n",
      "6       0.050241           0.026293  \n",
      "7       0.064091           0.065282  \n",
      "8       0.091547           0.077257  \n",
      "9       0.062755           0.073227       Feature_importances\n",
      "0              0.000000\n",
      "1              0.000000\n",
      "2            276.551521\n",
      "3             36.626670\n",
      "4             18.593889\n",
      "..                  ...\n",
      "76             0.000000\n",
      "77             1.727038\n",
      "78             0.748173\n",
      "79             0.000000\n",
      "80             2.513804\n",
      "\n",
      "[81 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import sys\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_Boosted(straekning, station, model_no, no_preds, data_name, both): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict_Boosted(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tbaseline = np.mean(y[train])\n",
    "\t\t\t\n",
    "\t\t\tboosted_model = estimator_tuple\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], y[train])\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, baseline, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\ty_true = []\n",
    "\t\tbaseline_pred = []\n",
    "\t\tfeature_importances = np.zeros(prediction_blocks[0][3].shape)\n",
    "\t\tfor i, (pred_block, test_indices, baseline, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\t\t\tbaseline_pred += [baseline]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_baseline = []\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_baseline.append(float(abs(y[test] - baseline_pred[fold])))\n",
    "\t\t\t\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\tbaseline_score = np.mean(cv_scores_baseline)\n",
    "\n",
    "\t\tresult_dictionary = {'y_true': y_true,'Predictions_full': avgfunc(predictions, axis=1), 'Predictions_baseline': baseline_pred, 'Error_full': full_score, 'Error_baseline': baseline_score, 'CV_score_full': cv_scores, 'CV_score_baseline': cv_scores_baseline}\n",
    "\n",
    "\t\t# Save results in .csv's\n",
    "\t\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\t\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\n",
    "\t\treturn result_df, fi_df\n",
    "\t\n",
    "\tdef custom_cross_val_predict_SARIMA_Boosted(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tbaseline = np.mean(y[train])\n",
    "\t\t\t\n",
    "\t\t\tarima_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\tarima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with ARIMA model\n",
    "\t\t\tarima_pred = arima_model.predict(n_periods=len(test))\n",
    "\t\t\tarima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\tarima_residuals_train = arima_model.predict_in_sample() - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, arima_pred_cap, baseline, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\tarima_pred = []\n",
    "\t\tbaseline_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros(prediction_blocks[0][4].shape)\n",
    "\t\tfor i, (pred_block, test_indices, arima_block, baseline, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tarima_pred.append(arima_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\tbaseline_pred += [baseline]\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tcv_scores = []\n",
    "\t\tcv_scores_arima = []\n",
    "\t\tcv_scores_baseline =[]\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_score = float(abs(y[test] - fold_predictions))\n",
    "\t\t\tfold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "\t\t\tcv_scores.append(fold_score)\n",
    "\t\t\tcv_scores_arima.append(fold_arima_score)\n",
    "\t\t\tcv_scores_baseline.append(float(abs(y[test] - baseline_pred[fold])))\n",
    "\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(cv_scores)\n",
    "\t\tarima_score = np.mean(cv_scores_arima)\n",
    "\t\tbaseline_score = np.mean(cv_scores_baseline)\n",
    "\t\tresult_dictionary = {'y_true': y_true,'Predictions_full': avgfunc(predictions, axis=1), 'Predictions_arima': np.array(arima_pred), 'Predictions_baseline': baseline_pred, 'Error_full': full_score, 'Error_arima': arima_score, 'Error_baseline': baseline_score, 'CV_score_full': cv_scores, 'CV_score_arima': cv_scores_arima, 'CV_score_baseline':cv_scores_baseline}\n",
    "\t\t# Save results in .csv's\n",
    "\t\tresult_df = pd.DataFrame(result_dictionary)\n",
    "\t\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\t\t\n",
    "\t\treturn result_df, fi_df\n",
    "\n",
    "\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/{}_data.csv\".format(data_name))\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tmodel_name = 'Xgboost'\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tmodel_name = 'Catboost'\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\tprint(f'Running... \\nBoth ARIMA and Boosted Random Forest: {both} \\nBoosted Random Forest Model: {model_name} \\nDataset: {data_name}\\nNo. predictions: {no_preds}')\n",
    "\tprint('----------------')\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\n",
    "\tif both:\n",
    "\t\tprint('HI')\n",
    "\t\tmodel_name = 'SARIMA_' + model_name\n",
    "\t\tmodel_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "\t\tbest_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(straekning), int(station)))]['Values'].values[0])\n",
    "\t\t# Define models\n",
    "\t\tarima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "\t\tresult_df, fi_df = custom_cross_val_predict_SARIMA_Boosted((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\telse:\n",
    "\t\tprint('HO')\n",
    "\t\tresult_df, fi_df = custom_cross_val_predict_Boosted((boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "\tprint(result_df, fi_df)\n",
    "\tresult_df.to_csv('Results/Separate_Runs/{}/{}/({}, {})_{}{}_results{}.csv'.format(data_name, model_name, straekning, station, model_name, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/{}/{}/({}, {})_{}{}_feature_importances{}.csv'.format(data_name, model_name, straekning, station, model_name, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]), data_name=sys.argv[5], both=int(sys.argv[6]))\n",
    "HPC_Boosted(straekning = 20, station = 19, model_no = 1, no_preds=10, data_name='Cleaned_simple_lagged', both=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All in one with x predictions into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running... \n",
      "Both ARIMA and Boosted Random Forest: True \n",
      "Boosted Random Forest Model: Catboost \n",
      "Dataset: Cleaned_simple_lagged\n",
      "No. predictions: 365\n",
      "----------------\n",
      "Initialize Expanding Window CV\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Fold 3\n",
      "Fold 4\n",
      "Fold 5\n",
      "Fold 6\n",
      "Fold 7\n",
      "Fold 8\n",
      "Fold 9\n",
      "Fold 10\n",
      "Fold 11\n",
      "Fold 12\n",
      "Fold 13\n",
      "Fold 14\n",
      "Fold 15\n",
      "Fold 16\n",
      "Fold 17\n",
      "Fold 18\n",
      "Fold 19\n",
      "Fold 20\n",
      "Fold 21\n",
      "Fold 22\n",
      "Fold 23\n",
      "Fold 24\n",
      "Fold 25\n",
      "Fold 26\n",
      "Fold 27\n",
      "Fold 28\n",
      "Fold 29\n",
      "Fold 30\n",
      "Fold 31\n",
      "Fold 32\n",
      "Fold 33\n",
      "Fold 34\n",
      "Fold 35\n",
      "Fold 36\n",
      "Fold 37\n",
      "Fold 38\n",
      "Fold 39\n",
      "Fold 40\n",
      "Fold 41\n",
      "Fold 42\n",
      "Fold 43\n",
      "Fold 44\n",
      "Fold 45\n",
      "Fold 46\n",
      "Fold 47\n",
      "Fold 48\n",
      "Fold 49\n",
      "Fold 50\n",
      "Fold 51\n",
      "Fold 52\n",
      "Fold 53\n",
      "Fold 54\n",
      "Fold 55\n",
      "Fold 56\n",
      "Fold 57\n",
      "Fold 58\n",
      "Fold 59\n",
      "Fold 60\n",
      "Fold 61\n",
      "Fold 62\n",
      "Fold 63\n",
      "Fold 64\n",
      "Fold 65\n",
      "Fold 66\n",
      "Fold 67\n",
      "Fold 68\n",
      "Fold 69\n",
      "Fold 70\n",
      "Fold 71\n",
      "Fold 72\n",
      "Fold 73\n",
      "Fold 74\n",
      "Fold 75\n",
      "Fold 76\n",
      "Fold 77\n",
      "Fold 78\n",
      "Fold 79\n",
      "Fold 80\n",
      "Fold 81\n",
      "Fold 82\n",
      "Fold 83\n",
      "Fold 84\n",
      "Fold 85\n",
      "Fold 86\n",
      "Fold 87\n",
      "Fold 88\n",
      "Fold 89\n",
      "Fold 90\n",
      "Fold 91\n",
      "Fold 92\n",
      "Fold 93\n",
      "Fold 94\n",
      "Fold 95\n",
      "Fold 96\n",
      "Fold 97\n",
      "Fold 98\n",
      "Fold 99\n",
      "Fold 100\n",
      "Fold 101\n",
      "Fold 102\n",
      "Fold 103\n",
      "Fold 104\n",
      "Fold 105\n",
      "Fold 106\n",
      "Fold 107\n",
      "Fold 108\n",
      "Fold 109\n",
      "Fold 110\n",
      "Fold 111\n",
      "Fold 112\n",
      "Fold 113\n",
      "Fold 114\n",
      "Fold 115\n",
      "Fold 116\n",
      "Fold 117\n",
      "Fold 118\n",
      "Fold 119\n",
      "Fold 120\n",
      "Fold 121\n",
      "Fold 122\n",
      "Fold 123\n",
      "Fold 124\n",
      "Fold 125\n",
      "Fold 126\n",
      "Fold 127\n",
      "Fold 128\n",
      "Fold 129\n",
      "Fold 130\n",
      "Fold 131\n",
      "Fold 132\n",
      "Fold 133\n",
      "Fold 134\n",
      "Fold 135\n",
      "Fold 136\n",
      "Fold 137\n",
      "Fold 138\n",
      "Fold 139\n",
      "Fold 140\n",
      "Fold 141\n",
      "Fold 142\n",
      "Fold 143\n",
      "Fold 144\n",
      "Fold 145\n",
      "Fold 146\n",
      "Fold 147\n",
      "Fold 148\n",
      "Fold 149\n",
      "Fold 150\n",
      "Fold 151\n",
      "Fold 152\n",
      "Fold 153\n",
      "Fold 154\n",
      "Fold 155\n",
      "Fold 156\n",
      "Fold 157\n",
      "Fold 158\n",
      "Fold 159\n",
      "Fold 160\n",
      "Fold 161\n",
      "Fold 162\n",
      "Fold 163\n",
      "Fold 164\n",
      "Fold 165\n",
      "Fold 166\n",
      "Fold 167\n",
      "Fold 168\n",
      "Fold 169\n",
      "Fold 170\n",
      "Fold 171\n",
      "Fold 172\n",
      "Fold 173\n",
      "Fold 174\n",
      "Fold 175\n",
      "Fold 176\n",
      "Fold 177\n",
      "Fold 178\n",
      "Fold 179\n",
      "Fold 180\n",
      "Fold 181\n",
      "Fold 182\n",
      "Fold 183\n",
      "Fold 184\n",
      "Fold 185\n",
      "Fold 186\n",
      "Fold 187\n",
      "Fold 188\n",
      "Fold 189\n",
      "Fold 190\n",
      "Fold 191\n",
      "Fold 192\n",
      "Fold 193\n",
      "Fold 194\n",
      "Fold 195\n",
      "Fold 196\n",
      "Fold 197\n",
      "Fold 198\n",
      "Fold 199\n",
      "Fold 200\n",
      "Fold 201\n",
      "Fold 202\n",
      "Fold 203\n",
      "Fold 204\n",
      "Fold 205\n",
      "Fold 206\n",
      "Fold 207\n",
      "Fold 208\n",
      "Fold 209\n",
      "Fold 210\n",
      "Fold 211\n",
      "Fold 212\n",
      "Fold 213\n",
      "Fold 214\n",
      "Fold 215\n",
      "Fold 216\n",
      "Fold 217\n",
      "Fold 218\n",
      "Fold 219\n",
      "Fold 220\n",
      "Fold 221\n",
      "Fold 222\n",
      "Fold 223\n",
      "Fold 224\n",
      "Fold 225\n",
      "Fold 226\n",
      "Fold 227\n",
      "Fold 228\n",
      "Fold 229\n",
      "Fold 230\n",
      "Fold 231\n",
      "Fold 232\n",
      "Fold 233\n",
      "Fold 234\n",
      "Fold 235\n",
      "Fold 236\n",
      "Fold 237\n",
      "Fold 238\n",
      "Fold 239\n",
      "Fold 240\n",
      "Fold 241\n",
      "Fold 242\n",
      "Fold 243\n",
      "Fold 244\n",
      "Fold 245\n",
      "Fold 246\n",
      "Fold 247\n",
      "Fold 248\n",
      "Fold 249\n",
      "Fold 250\n",
      "Fold 251\n",
      "Fold 252\n",
      "Fold 253\n",
      "Fold 254\n",
      "Fold 255\n",
      "Fold 256\n",
      "Fold 257\n",
      "Fold 258\n",
      "Fold 259\n",
      "Fold 260\n",
      "Fold 261\n",
      "Fold 262\n",
      "Fold 263\n",
      "Fold 264\n",
      "Fold 265\n",
      "Fold 266\n",
      "Fold 267\n",
      "Fold 268\n",
      "Fold 269\n",
      "Fold 270\n",
      "Fold 271\n",
      "Fold 272\n",
      "Fold 273\n",
      "Fold 274\n",
      "Fold 275\n",
      "Fold 276\n",
      "Fold 277\n",
      "Fold 278\n",
      "Fold 279\n",
      "Fold 280\n",
      "Fold 281\n",
      "Fold 282\n",
      "Fold 283\n",
      "Fold 284\n",
      "Fold 285\n",
      "Fold 286\n",
      "Fold 287\n",
      "Fold 288\n",
      "Fold 289\n",
      "Fold 290\n",
      "Fold 291\n",
      "Fold 292\n",
      "Fold 293\n",
      "Fold 294\n",
      "Fold 295\n",
      "Fold 296\n",
      "Fold 297\n",
      "Fold 298\n",
      "Fold 299\n",
      "Fold 300\n",
      "Fold 301\n",
      "Fold 302\n",
      "Fold 303\n",
      "Fold 304\n",
      "Fold 305\n",
      "Fold 306\n",
      "Fold 307\n",
      "Fold 308\n",
      "Fold 309\n",
      "Fold 310\n",
      "Fold 311\n",
      "Fold 312\n",
      "Fold 313\n",
      "Fold 314\n",
      "Fold 315\n",
      "Fold 316\n",
      "Fold 317\n",
      "Fold 318\n",
      "Fold 319\n",
      "Fold 320\n",
      "Fold 321\n",
      "Fold 322\n",
      "Fold 323\n",
      "Fold 324\n",
      "Fold 325\n",
      "Fold 326\n",
      "Fold 327\n",
      "Fold 328\n",
      "Fold 329\n",
      "Fold 330\n",
      "Fold 331\n",
      "Fold 332\n",
      "Fold 333\n",
      "Fold 334\n",
      "Fold 335\n",
      "Fold 336\n",
      "Fold 337\n",
      "Fold 338\n",
      "Fold 339\n",
      "Fold 340\n",
      "Fold 341\n",
      "Fold 342\n",
      "Fold 343\n",
      "Fold 344\n",
      "Fold 345\n",
      "Fold 346\n",
      "Fold 347\n",
      "Fold 348\n",
      "Fold 349\n",
      "Fold 350\n",
      "Fold 351\n",
      "Fold 352\n",
      "Fold 353\n",
      "Fold 354\n",
      "Fold 355\n",
      "Fold 356\n",
      "Fold 357\n",
      "Fold 358\n",
      "Fold 359\n",
      "Fold 360\n",
      "Fold 361\n",
      "Fold 362\n",
      "Fold 363\n",
      "Fold 364\n"
     ]
    }
   ],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "import pmdarima as pm\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_Boosted(straekning, station, model_no, no_preds, data_name, both, horizon): #sys.argv[1] sys.argv[2]\n",
    "\t# Customized cross validation with rolling window and XGboost\n",
    "\tdef custom_cross_val_predict_Boosted(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555, horizon=1):\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555, h=1):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=h)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tbaseline = [np.mean(y[train])] * len(y[test])\n",
    "\t\t\t\n",
    "\t\t\tboosted_model = estimator_tuple\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], y[train])\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.clip(preds, 0, 1) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, baseline, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial, h=horizon)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\ty_true = []\n",
    "\t\tbaseline_pred = []\n",
    "\t\tfeature_importances = np.zeros(prediction_blocks[0][3].shape)\n",
    "\t\tfor i, (pred_block, test_indices, baseline, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\ty_true += [y[test_indices]]\n",
    "\t\t\tbaseline_pred += [baseline]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tresult_df = pd.DataFrame()\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_dict = {}\n",
    "\t\t\tfor i in range(horizon):\n",
    "\t\t\t\tfold_score = abs(y[test][i] - fold_predictions[i])\n",
    "\t\t\t\tfold_score_baseline = abs(y[test][i] - baseline_pred[fold][i])\n",
    "\t\t\t\tfold_dict[f'y_true{i}'] = y[test][i]\n",
    "\t\t\t\tfold_dict[f'Predictions_full{i}'] = fold_predictions[i]\n",
    "\t\t\t\tfold_dict[f'Predictions_baseline{i}'] = baseline_pred[fold][i]\n",
    "\t\t\t\tfold_dict[f'CV_score_full{i}'] = fold_score\n",
    "\t\t\t\tfold_dict[f'CV_score_baseline{i}'] = fold_score_baseline\n",
    "\t\t\tresult_df = pd.concat([result_df, pd.DataFrame(fold_dict, index=[0])])\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(result_df.iloc[:,result_df.columns.str.startswith('CV_score_full')], axis=0)\n",
    "\t\tbaseline_score = np.mean(result_df.iloc[:,result_df.columns.str.startswith('CV_score_baseline')], axis=0)\n",
    "\t\tfor i in range(horizon):\n",
    "\t\t\tresult_df[f'Error_full{i}'] = full_score[i]\n",
    "\t\t\tresult_df[f'Error_baseline{i}'] = baseline_score[i]\n",
    "\n",
    "\n",
    "\t\t# Save results in .csv's\n",
    "\t\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\n",
    "\t\treturn result_df, fi_df\n",
    "\t\n",
    "\tdef custom_cross_val_predict_SARIMA_Boosted(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555, horizon=1):\n",
    "\n",
    "\t\tdef indexable(*iterables):\n",
    "\t\t\t\"\"\"Internal utility to handle input types\"\"\"\n",
    "\t\t\tresults = []\n",
    "\t\t\tfor iterable in iterables:\n",
    "\t\t\t\tif not hasattr(iterable, \"__iter__\"):\n",
    "\t\t\t\t\traise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "\t\t\t\tresults.append(iterable)\n",
    "\t\t\treturn results\n",
    "\n",
    "\t\tdef check_cv(cv, initial = 2555, h=1):\n",
    "\t\t\t\"\"\"Internal utility to check cv\"\"\"\n",
    "\t\t\tif cv is None:\n",
    "\t\t\t\tcv = RollingForecastCV(initial=initial, step=1, h=h)\n",
    "\t\t\treturn cv\n",
    "\n",
    "\t\tdef check_endog(y, copy=True, preserve_series=False):\n",
    "\t\t\t\"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "\t\t\tfrom pmdarima.utils import check_endog\n",
    "\t\t\treturn check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "\t\tdef _check_averaging(averaging):\n",
    "\t\t\t\"\"\"Internal utility to check averaging\"\"\"\n",
    "\t\t\tif averaging == \"mean\":\n",
    "\t\t\t\treturn np.nanmean\n",
    "\t\t\telif averaging == \"median\":\n",
    "\t\t\t\treturn np.nanmedian\n",
    "\t\t\telif callable(averaging):\n",
    "\t\t\t\treturn averaging\n",
    "\t\t\telse:\n",
    "\t\t\t\traise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "\t\tdef _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "\t\t\tprint('Fold {}'.format(fold))\n",
    "\t\t\t\"\"\"Internal utility to fit and predict\"\"\"\n",
    "\t\t\tbaseline = [np.mean(y[train])] * len(y[test])\n",
    "\t\t\t\n",
    "\t\t\tarima_model, boosted_model = estimator_tuple\n",
    "\t\t\t# Fit ARIMA model\n",
    "\t\t\tarima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "\t\t\t# Predict with ARIMA model\n",
    "\t\t\tarima_pred = arima_model.predict(n_periods=len(test))\n",
    "\t\t\tarima_pred_cap = np.clip(arima_pred, 0, 1)\n",
    "\t\t\t# Calculate residuals for RF input\n",
    "\t\t\tarima_residuals_train = arima_model.predict_in_sample() - y[train]\n",
    "\t\t\t# Train the model\n",
    "\t\t\tboosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "\t\t\t# Predict the labels of the test set\n",
    "\t\t\t#preds = model.predict(D_test)\n",
    "\t\t\tpreds = boosted_model.predict(X.iloc[test,1:])\n",
    "\t\t\t# Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "\t\t\toverall_pred = np.clip(arima_pred - preds, 0,1) # make sure it is in [0;1]\n",
    "\t\t\treturn overall_pred, test, arima_pred_cap, baseline, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "\t\ty, X = indexable(y, X)\n",
    "\t\ty = check_endog(y, copy=False, preserve_series=True)\n",
    "\t\tcv = check_cv(cv, initial, h=horizon)\n",
    "\t\tavgfunc = _check_averaging(averaging)\n",
    "\n",
    "\t\tif cv.step > cv.horizon:\n",
    "\t\t\traise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "\t\tprediction_blocks = [\n",
    "\t\t\t_fit_and_predict(fold,\n",
    "\t\t\t\t\t\t\t estimator,\n",
    "\t\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t\t X,\n",
    "\t\t\t\t\t\t\t train=train,\n",
    "\t\t\t\t\t\t\t test=test,\n",
    "\t\t\t\t\t\t\t verbose=verbose,)  # TODO: fit params?\n",
    "\t\t\tfor fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "\t\tpred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "\t\tarima_pred = []\n",
    "\t\tbaseline_pred = []\n",
    "\t\ty_true = []\n",
    "\t\tfeature_importances = np.zeros(prediction_blocks[0][4].shape)\n",
    "\t\tfor i, (pred_block, test_indices, arima_block, baseline, feature_importance) in enumerate(prediction_blocks):\n",
    "\t\t\tpred_matrix[test_indices, i] = pred_block\n",
    "\t\t\tarima_pred.append(arima_block)\n",
    "\t\t\tfeature_importances += feature_importance\n",
    "\t\t\tbaseline_pred += [baseline]\n",
    "\t\t\ty_true += [y[test_indices][0]]\n",
    "\n",
    "\n",
    "\t\tif return_raw_predictions:\n",
    "\t\t\tpredictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "\t\t\tfor pred_block, test_indices in prediction_blocks:\n",
    "\t\t\t\tpredictions[test_indices[0]] = pred_block\n",
    "\t\t\treturn predictions\n",
    "\n",
    "\t\ttest_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "\t\tpredictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\t\t# Calculate CV score\n",
    "\t\tresult_df = pd.DataFrame()\n",
    "\t\tfor fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "\t\t\tfold_predictions = pred_matrix[test, fold]\n",
    "\t\t\tfold_dict = {}\n",
    "\t\t\tfor i in range(horizon):\n",
    "\t\t\t\tfold_score = abs(y[test][i] - fold_predictions[i])\n",
    "\t\t\t\tfold_score_baseline = abs(y[test][i] - baseline_pred[fold][i])\n",
    "\t\t\t\tfold_score_arima = abs(y[test][i] - arima_pred[fold][i])\n",
    "\t\t\t\tfold_dict[f'y_true{i}'] = y[test][i]\n",
    "\t\t\t\tfold_dict[f'Predictions_full{i}'] = fold_predictions[i]\n",
    "\t\t\t\tfold_dict[f'Predictions_baseline{i}'] = baseline_pred[fold][i]\n",
    "\t\t\t\tfold_dict[f'Predictions_ARIMA{i}'] = arima_pred[fold][i]\n",
    "\t\t\t\tfold_dict[f'CV_score_full{i}'] = fold_score\n",
    "\t\t\t\tfold_dict[f'CV_score_baseline{i}'] = fold_score_baseline\n",
    "\t\t\t\tfold_dict[f'CV_score_ARIMA{i}'] = fold_score_arima\n",
    "\t\t\tresult_df = pd.concat([result_df, pd.DataFrame(fold_dict, index=[0])])\n",
    "\t\t# Compute overall CV score\n",
    "\t\tfull_score = np.mean(result_df.iloc[:,result_df.columns.str.startswith('CV_score_full')], axis=0)\n",
    "\t\tbaseline_score = np.mean(result_df.iloc[:,result_df.columns.str.startswith('CV_score_baseline')], axis=0)\n",
    "\t\tarima_score = np.mean(result_df.iloc[:,result_df.columns.str.startswith('CV_score_ARIMA')], axis=0)\n",
    "\t\tfor i in range(horizon):\n",
    "\t\t\tresult_df[f'Error_full{i}'] = full_score[i]\n",
    "\t\t\tresult_df[f'Error_baseline{i}'] = baseline_score[i]\n",
    "\t\t\tresult_df[f'Error_ARIMA{i}'] = arima_score[i]\n",
    "\t\t\t\n",
    "\t\tfi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "\t\t\n",
    "\t\treturn result_df, fi_df\n",
    "\n",
    "\n",
    "\n",
    "\t# Extract data\n",
    "\tdata = pd.read_csv(\"Data/{}_data.csv\".format(data_name))\n",
    "\ty = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "\tX = data[(data['visualiseringskode'] == straekning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "\t# Define models\n",
    "\tmodel_name = 'Xgboost'\n",
    "\tboosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\tif model_no: # 0=Xgboost, 1=Catboost\n",
    "\t\tmodel_name = 'Catboost'\n",
    "\t\tboosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "\tprint(f'Running... \\nBoth ARIMA and Boosted Random Forest: {both} \\nBoosted Random Forest Model: {model_name} \\nDataset: {data_name}\\nNo. predictions: {no_preds}')\n",
    "\tprint('----------------')\n",
    "\t# Expanding Window CV\n",
    "\tprint('Initialize Expanding Window CV')\n",
    "\tinitial_start = y.shape[0] - no_preds\n",
    "\n",
    "\tif both:\n",
    "\t\tmodel_name = 'ARIMA_' + model_name\n",
    "\t\tmodel_params_arima = pd.read_csv('Data/Best_model_parameters_ARIMA_route_station.csv')\n",
    "\t\tbest_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(straekning), int(station)))]['Values'].values[0])\n",
    "\t\t# Define models\n",
    "\t\tarima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "\t\tresult_df, fi_df = custom_cross_val_predict_SARIMA_Boosted((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start, horizon=horizon)\n",
    "\t\t\n",
    "\telse:\n",
    "\t\tresult_df, fi_df = custom_cross_val_predict_Boosted((boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start, horizon=horizon)\n",
    "\t\n",
    "\tresult_df.to_csv('Results/Separate_Runs/{}/{}/({}, {})_Boosted{}_results{}.csv'.format(data_name, model_name, straekning, station, model_no, no_preds), index=False)\n",
    "\tfi_df.to_csv('Results/Separate_Runs/{}/{}/({}, {})_Boosted{}_feature_importances{}.csv'.format(data_name, model_name, straekning, station, model_no, no_preds), index=False)\n",
    "\n",
    "#HPC_Boosted(straekning = int(sys.argv[1]), station = int(sys.argv[2]), model_no = int(sys.argv[3]), no_preds=int(sys.argv[4]), data_name=sys.argv[5], both=sys.argv[6])\n",
    "HPC_Boosted(straekning = 20, station = 19, model_no = 1, no_preds=365, data_name='Cleaned_simple_lagged', both=True, horizon=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [[2,0],\n",
    "[2,2],\n",
    "[9,2],\n",
    "[10,3],\n",
    "[24,3],\n",
    "[21,4],\n",
    "[10,5],\n",
    "[8,5],\n",
    "[11,6],\n",
    "[13,6],\n",
    "[9,6],\n",
    "[2,8],\n",
    "[20,9],\n",
    "[17,11],\n",
    "[5,12],\n",
    "[5,13],\n",
    "[6,13],\n",
    "[7,13],\n",
    "[19,14],\n",
    "[17,15],\n",
    "[11,16],\n",
    "[19,17],\n",
    "[20,17],\n",
    "[21,17],\n",
    "[27,17],\n",
    "[23,18],\n",
    "[20,19],\n",
    "[15,20],\n",
    "[16,20],\n",
    "[2,21],\n",
    "[3,21],\n",
    "[18,22],\n",
    "[23,22],\n",
    "[25,22],\n",
    "[25,23],\n",
    "[13,24],\n",
    "[15,24],\n",
    "[13,25],\n",
    "[14,25],\n",
    "[20,26],\n",
    "[11,27],\n",
    "[24,28],\n",
    "[14,29],\n",
    "[16,31],\n",
    "[18,31],\n",
    "[27,31],\n",
    "[16,33],\n",
    "[17,33],\n",
    "[19,33],\n",
    "[6,34],\n",
    "[9,34],\n",
    "[12,35],\n",
    "[16,36],\n",
    "[7,37],\n",
    "[8,37],\n",
    "[6,38],\n",
    "[3,39],\n",
    "[4,39],\n",
    "[5,39],\n",
    "[14,40],\n",
    "[24,41],\n",
    "[11,42],\n",
    "[12,42],\n",
    "[4,43],\n",
    "[8,44],\n",
    "[3,45],\n",
    "[5,46],\n",
    "[9,46],\n",
    "[25,47]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSB_Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
