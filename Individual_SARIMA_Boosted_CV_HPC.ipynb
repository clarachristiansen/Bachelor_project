{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no = 0, no_preds=30): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPC_SARIMA_Boosted(strækning = 1, station = 0, model_no=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.model_selection import RollingForecastCV\n",
    "import numpy as np\n",
    "\n",
    "# Customized cross validation with rolling window and XGboost\n",
    "def custom_cross_val_predict(estimator, y, X=None, cv=None, verbose=0, averaging=\"mean\", return_raw_predictions=False, initial=2555):\n",
    "    \"\"\"Generate cross-validated estimates for each input data point\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : tuple\n",
    "        A tuple containing two estimators. The first estimator should be the ARIMA model\n",
    "        and the second one should be the Random Forest model.\n",
    "\n",
    "    y : array-like or iterable, shape=(n_samples,)\n",
    "        The time-series array.\n",
    "\n",
    "    X : array-like, shape=[n_obs, n_vars], optional (default=None)\n",
    "        An optional 2-d array of exogenous variables.\n",
    "\n",
    "    cv : BaseTSCrossValidator or None, optional (default=None)\n",
    "        An instance of cross-validation. If None, will use a RollingForecastCV.\n",
    "        Note that for cross-validation predictions, the CV step cannot exceed\n",
    "        the CV horizon, or there will be a gap between fold predictions.\n",
    "\n",
    "    verbose : integer, optional\n",
    "        The verbosity level.\n",
    "\n",
    "    averaging : str or callable, one of [\"median\", \"mean\"] (default=\"mean\")\n",
    "        Unlike normal CV, time series CV might have different folds (windows)\n",
    "        forecasting the same time step. After all forecast windows are made,\n",
    "        we build a matrix of y x n_folds, populating each fold's forecasts like\n",
    "        so::\n",
    "\n",
    "            nan nan nan  # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "              1 nan nan  # test samples\n",
    "              4   3 nan\n",
    "              3 2.5 3.5\n",
    "            nan   6   5\n",
    "            nan nan   4\n",
    "\n",
    "        We then average each time step's forecasts to end up with our final\n",
    "        prediction results.\n",
    "\n",
    "    return_raw_predictions : bool (default=False)\n",
    "        If True, raw predictions are returned instead of averaged ones.\n",
    "        This results in a y x h matrix. For example, if h=3, and step=1 then:\n",
    "\n",
    "            nan nan nan # training samples\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "            1   4   2   # test samples\n",
    "            2   5   7\n",
    "            8   9   1\n",
    "            nan nan nan\n",
    "            nan nan nan\n",
    "\n",
    "        First column contains all one-step-ahead-predictions, second column all\n",
    "        two-step-ahead-predictions etc. Further metrics can then be calculated\n",
    "        as desired.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like, shape=(n_samples,)\n",
    "        The predicted values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def indexable(*iterables):\n",
    "        \"\"\"Internal utility to handle input types\"\"\"\n",
    "        results = []\n",
    "        for iterable in iterables:\n",
    "            if not hasattr(iterable, \"__iter__\"):\n",
    "                raise ValueError(\"Input {!r} is not indexable\".format(iterable))\n",
    "            results.append(iterable)\n",
    "        return results\n",
    "\n",
    "    def check_cv(cv, initial = 2555):\n",
    "        \"\"\"Internal utility to check cv\"\"\"\n",
    "        if cv is None:\n",
    "            cv = RollingForecastCV(initial=initial, step=1, h=1)\n",
    "        return cv\n",
    "\n",
    "    def check_endog(y, copy=True, preserve_series=False):\n",
    "        \"\"\"Internal utility to check endogenous variable\"\"\"\n",
    "        from pmdarima.utils import check_endog\n",
    "        return check_endog(y, copy=copy, preserve_series=preserve_series)\n",
    "\n",
    "    def _check_averaging(averaging):\n",
    "        \"\"\"Internal utility to check averaging\"\"\"\n",
    "        if averaging == \"mean\":\n",
    "            return np.nanmean\n",
    "        elif averaging == \"median\":\n",
    "            return np.nanmedian\n",
    "        elif callable(averaging):\n",
    "            return averaging\n",
    "        else:\n",
    "            raise ValueError(\"Unknown averaging method: {}\".format(averaging))\n",
    "\n",
    "    def _fit_and_predict(fold, estimator_tuple, y, X, train, test, verbose=1):\n",
    "        \"\"\"Internal utility to fit and predict\"\"\"\n",
    "        arima_model, boosted_model = estimator_tuple\n",
    "        # Fit ARIMA model\n",
    "        arima_model.fit(y[train]) # X=X.iloc[train, :]\n",
    "        # Predict with ARIMA model\n",
    "        arima_pred = arima_model.predict(n_periods=len(test))\n",
    "        arima_pred_cap = max(min(1, arima_pred[0]), 0)\n",
    "        # Calculate residuals for RF input\n",
    "        arima_residuals_train = arima_pred - y[train]\n",
    "\n",
    "        #model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster = 'gbtree', max_depth=5, steps =20, learning_rate=0.1) # 'reg:squarederror'\n",
    "        # Train the model\n",
    "        #model = model.fit(D_train, steps, watchlist)\n",
    "        boosted_model = boosted_model.fit(X.iloc[train,1:], arima_residuals_train)\n",
    "        # Predict the labels of the test set\n",
    "        #preds = model.predict(D_test)\n",
    "        preds = boosted_model.predict(X.iloc[test,1:])\n",
    "        # Overall prediction residuals = pred - true <=> true = pred - residuals\n",
    "        overall_pred = np.array(max(min(1, arima_pred[0] - preds), 0)) # make sure it is in [0;1]\n",
    "\n",
    "        return overall_pred, test, arima_pred_cap, boosted_model.feature_importances_ #arima_residuals_test\n",
    "\n",
    "    y, X = indexable(y, X)\n",
    "    y = check_endog(y, copy=False, preserve_series=True)\n",
    "    cv = check_cv(cv, initial)\n",
    "    avgfunc = _check_averaging(averaging)\n",
    "\n",
    "    if cv.step > cv.horizon:\n",
    "        raise ValueError(\"CV step cannot be > CV horizon, or there will be a gap in predictions between folds\")\n",
    "\n",
    "    prediction_blocks = [\n",
    "        _fit_and_predict(fold,\n",
    "                         estimator,\n",
    "                         y,\n",
    "                         X,\n",
    "                         train=train,\n",
    "                         test=test,\n",
    "                         verbose=verbose,)  # TODO: fit params?\n",
    "        for fold, (train, test) in enumerate(cv.split(y, X))]\n",
    "\n",
    "    pred_matrix = np.ones((y.shape[0], len(prediction_blocks))) * np.nan\n",
    "    arima_pred = []\n",
    "    feature_importances = np.zeros((211,))\n",
    "    for i, (pred_block, test_indices, arima_block, feature_importance) in enumerate(prediction_blocks):\n",
    "        pred_matrix[test_indices, i] = pred_block\n",
    "        arima_pred.append(arima_block)\n",
    "        feature_importances += feature_importance\n",
    "\n",
    "\n",
    "    if return_raw_predictions:\n",
    "        predictions = np.ones((y.shape[0], cv.horizon)) * np.nan\n",
    "        for pred_block, test_indices in prediction_blocks:\n",
    "            predictions[test_indices[0]] = pred_block\n",
    "        return predictions\n",
    "\n",
    "    test_mask = ~(np.isnan(pred_matrix).all(axis=1))\n",
    "    predictions = pred_matrix[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate CV score\n",
    "    cv_scores = []\n",
    "    cv_scores_arima = []\n",
    "    for fold, (train, test) in enumerate(cv.split(y, X)):\n",
    "        fold_predictions = pred_matrix[test, fold]\n",
    "        fold_score = float(abs(y[test] - fold_predictions))\n",
    "        fold_arima_score = float(abs(y[test] - arima_pred[fold]))\n",
    "        cv_scores.append(fold_score)\n",
    "        cv_scores_arima.append(fold_arima_score)\n",
    "\n",
    "    # Compute overall CV score\n",
    "    full_score = np.mean(cv_scores)\n",
    "    arima_score = np.mean(cv_scores_arima)\n",
    "\n",
    "    return avgfunc(predictions, axis=1), np.array(arima_pred), full_score,  arima_score, cv_scores, cv_scores_arima, feature_importances\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pmdarima as pm\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "import catboost as cb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def HPC_SARIMA_Boosted(strækning = sys.argv[1], station = sys.argv[2], model_no = sys.argv[3], no_preds=sys.argv[4]): #sys.argv[1] sys.argv[2]\n",
    "    # Extract data\n",
    "    data = pd.read_csv(\"Data/Cleaned_data.csv\")\n",
    "    y = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)]['togpunktlighed'].values\n",
    "    X = data[(data['visualiseringskode'] == strækning) & (data['station'] == station)].iloc[:,1:]\n",
    "\n",
    "    # Best model parameters for ARIMA\n",
    "    model_params_arima = pd.read_csv('Data/Best_model_parameters_SARIMA_strækning_station.csv')\n",
    "    best_arima_params = ast.literal_eval(model_params_arima[model_params_arima['Key'] == str((int(strækning), int(station)))]['Values'][0])\n",
    "\n",
    "    # Define models\n",
    "    arima_model = pm.arima.ARIMA(order = best_arima_params[0], seasonal_order=best_arima_params[1])\n",
    "    if model_no: # 0=Xgboost, 1=Catboost\n",
    "        boosted_model = cb.CatBoostRegressor(objective = 'MAE', iterations=20, learning_rate=0.1, max_depth=5, verbose=0)  \n",
    "    boosted_model = xgb.XGBRegressor(objective = 'reg:absoluteerror', booster='gbtree', steps=20, learning_rate=0.1, max_depth=5) \n",
    "\n",
    "    # Expanding Window CV\n",
    "    initial_start = y.shape[0] - no_preds\n",
    "    pred_full, pred_arima, error_full, error_arima, cv_score_full, cv_score_arima, feature_importances = custom_cross_val_predict((arima_model, boosted_model), y, X, cv=None, verbose=1, averaging=\"mean\", return_raw_predictions=False, initial=initial_start)\n",
    "    result_dictionary = {'Predictions_full': pred_full, 'Predictions_arima': pred_arima, 'Error_full': error_full, 'Error_arima': error_arima, 'CV_score_full': cv_score_full, \n",
    "                  'CV_score_arima': cv_score_arima}\n",
    "    \n",
    "    # Save results in .csv's\n",
    "    result_df = pd.DataFrame(result_dictionary)\n",
    "    fi_df = pd.DataFrame(feature_importances, columns=['Feature_importances'])\n",
    "    result_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_results.csv', index=False)\n",
    "    fi_df.to_csv(f'Results/({strækning}, {station})_SARIMA_Boosted{model_no}_feature_importances.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n",
      "/bin/sh: bsub: command not found\n"
     ]
    }
   ],
   "source": [
    "import glob,subprocess\n",
    "import numpy as np  # Ensure numpy is imported\n",
    "combinations = [[1,0], [1,1], [2,0]]\n",
    "cmd = '''hi\n",
    "safa'''\n",
    "for straekning, station in combinations:\n",
    "    cmdi = cmd + f' {straekning} {station} 0 30'\n",
    "    proc = subprocess.Popen('bsub ', stdin=subprocess.PIPE, shell=True)\n",
    "    proc.communicate(bytes(cmdi, encoding='UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval('[(3, 1, 5), (0, 0, 0, 0)]')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSB_Bachelorprojekt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
